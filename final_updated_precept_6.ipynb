{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of six different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8b0f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd4d8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base settings\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "n_informative = 10\n",
    "n_redundant = 10\n",
    "n_classes = 5\n",
    "random_state = 42\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative,\n",
    "                           n_redundant=n_redundant, n_clusters_per_class=1, n_classes=n_classes,\n",
    "                           random_state=random_state)\n",
    "\n",
    "# Add noise features\n",
    "np.random.seed(random_state)\n",
    "noise = np.random.normal(size=(n_samples, n_features - n_informative - n_redundant))\n",
    "X = np.hstack((X, noise))\n",
    "\n",
    "# Feature names\n",
    "feature_names = [f'feature_{i}' for i in range(1, n_features + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42b3bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "nb = GaussianNB()\n",
    "log_reg = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n",
    "svm = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66a8a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.49 ± 0.04\n",
      "Naive Bayes Accuracy: 0.67 ± 0.01\n",
      "Logistic Regression Accuracy: 0.56 ± 0.02\n",
      "SVM Accuracy: 0.59 ± 0.03\n",
      "Decision Tree Accuracy: 0.56 ± 0.04\n",
      "Random Forest Accuracy: 0.73 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models using cross-validation\n",
    "models = [knn, nb, log_reg, svm, dt, rf]\n",
    "model_names = ['KNN', 'Naive Bayes', 'Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "scores = {name: cross_val_score(model, X, y, cv=5) for name, model in zip(model_names, models)}\n",
    "for name, score in scores.items():\n",
    "    print(f\"{name} Accuracy: {np.mean(score):.2f} ± {np.std(score):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need to select feature? (How do you justify your method in research?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dicsuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_knn(X, y, k=10):\n",
    "    vt = VarianceThreshold(threshold=0.1)\n",
    "    X_vt = vt.fit_transform(X)\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X_vt, y)\n",
    "    knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "    return X_selected, knn\n",
    "\n",
    "# 2. Naive Bayes with SelectKBest\n",
    "def select_features_nb(X, y, k=10):\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    nb = GaussianNB()\n",
    "    return X_selected, nb\n",
    "\n",
    "# 3. Logistic Regression with RFE\n",
    "def select_features_logreg(X, y, n_features_to_select=10):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    log_reg = LogisticRegression(random_state=42)\n",
    "    selector = RFE(estimator=log_reg, n_features_to_select=n_features_to_select)\n",
    "    X_selected = selector.fit_transform(X_scaled, y)\n",
    "    final_log_reg = LogisticRegression(random_state=42)\n",
    "    return X_selected, final_log_reg\n",
    "\n",
    "# 4. SVM with RFE\n",
    "def select_features_svm(X, y, n_features_to_select=10):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    svm = SVC(kernel='linear', random_state=42)\n",
    "    selector = RFE(estimator=svm, n_features_to_select=n_features_to_select)\n",
    "    X_selected = selector.fit_transform(X_scaled, y)\n",
    "    final_svm = SVC(kernel='linear', random_state=42)\n",
    "    return X_selected, final_svm\n",
    "\n",
    "# 5. Decision Tree with built-in feature importance\n",
    "def select_features_dt(X, y, threshold=0.01):\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X, y)\n",
    "    importances = dt.feature_importances_\n",
    "    selected_features = importances > threshold\n",
    "    X_selected = X[:, selected_features]\n",
    "    final_dt = DecisionTreeClassifier(random_state=42)\n",
    "    return X_selected, final_dt\n",
    "\n",
    "# 6. Random Forest with built-in feature importance\n",
    "def select_features_rf(X, y, threshold=0.01):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    selected_features = importances > threshold\n",
    "    X_selected = X[:, selected_features]\n",
    "    final_rf = RandomForestClassifier(random_state=42)\n",
    "    return X_selected, final_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of `f_classif` and `RFE`\n",
    "\n",
    "#### `f_classif` (F-Classification)\n",
    "`f_classif` is a scoring function used in feature selection methods like `SelectKBest`. It performs an ANOVA F-test to evaluate the relationship between each feature and the target variable. This method assumes that higher F-values indicate that the feature provides more significant discriminatory power for the target classes. Essentially, `f_classif` helps to select the features that have the strongest relationship with the target.\n",
    "\n",
    "- **How it works**: \n",
    "  - It computes the variance between groups (based on the target classes) and compares it to the variance within groups (i.e., within the same class).\n",
    "  - Features with larger between-group variance are considered more useful for classification tasks.\n",
    "  \n",
    "#### `RFE` (Recursive Feature Elimination)\n",
    "`RFE` is a feature selection method that recursively removes less important features to reduce the feature set size. The algorithm starts by training a model (such as Logistic Regression or SVM) on the full feature set. It then ranks the importance of each feature according to the model's coefficients or importance scores. The least important feature is removed, and the process is repeated until the desired number of features is left.\n",
    "\n",
    "- **How it works**:\n",
    "  1. The model is trained on all features.\n",
    "  2. Features are ranked based on their importance (e.g., the magnitude of their coefficients in Logistic Regression).\n",
    "  3. The least important feature is removed, and the model is retrained on the remaining features.\n",
    "  4. Steps 2-3 are repeated until the desired number of features is selected.\n",
    "\n",
    "- **Use case**: `RFE` is particularly useful when you want to optimize model performance by retaining only the most relevant features and reducing the computational cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after feature selection:\n",
      "KNN Accuracy: 0.77 ± 0.03\n",
      "Naive Bayes Accuracy: 0.64 ± 0.02\n",
      "Logistic Regression Accuracy: 0.73 ± 0.04\n",
      "SVM Accuracy: 0.81 ± 0.01\n",
      "Decision Tree Accuracy: 0.63 ± 0.03\n",
      "Random Forest Accuracy: 0.81 ± 0.02\n",
      "\n",
      "Comparison with original results:\n",
      "KNN:\n",
      "  Original: 0.49 ± 0.04\n",
      "  After feature selection: 0.77 ± 0.03\n",
      "  Difference: 0.28\n",
      "\n",
      "Naive Bayes:\n",
      "  Original: 0.67 ± 0.01\n",
      "  After feature selection: 0.64 ± 0.02\n",
      "  Difference: -0.03\n",
      "\n",
      "Logistic Regression:\n",
      "  Original: 0.56 ± 0.02\n",
      "  After feature selection: 0.73 ± 0.04\n",
      "  Difference: 0.17\n",
      "\n",
      "SVM:\n",
      "  Original: 0.59 ± 0.03\n",
      "  After feature selection: 0.81 ± 0.01\n",
      "  Difference: 0.21\n",
      "\n",
      "Decision Tree:\n",
      "  Original: 0.56 ± 0.04\n",
      "  After feature selection: 0.63 ± 0.03\n",
      "  Difference: 0.07\n",
      "\n",
      "Random Forest:\n",
      "  Original: 0.73 ± 0.02\n",
      "  After feature selection: 0.81 ± 0.02\n",
      "  Difference: 0.09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the original models\n",
    "knn = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "nb = GaussianNB()\n",
    "log_reg = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\n",
    "svm = make_pipeline(StandardScaler(), SVC(kernel='linear', random_state=42))\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "models = [knn, nb, log_reg, svm, dt, rf]\n",
    "model_names = ['KNN', 'Naive Bayes', 'Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest']\n",
    "\n",
    "# Evaluate the original models\n",
    "original_scores = {}\n",
    "for name, model in zip(model_names, models):\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    original_scores[name] = scores\n",
    "\n",
    "# Apply feature selection and create new models\n",
    "X_knn, knn = select_features_knn(X, y)\n",
    "X_nb, nb = select_features_nb(X, y)\n",
    "X_log_reg, log_reg = select_features_logreg(X, y)\n",
    "X_svm, svm = select_features_svm(X, y)\n",
    "X_dt, dt = select_features_dt(X, y)\n",
    "X_rf, rf = select_features_rf(X, y)\n",
    "\n",
    "# Create a dictionary to store the selected features and corresponding models\n",
    "selected_data = {\n",
    "    'KNN': (X_knn, knn),\n",
    "    'Naive Bayes': (X_nb, nb),\n",
    "    'Logistic Regression': (X_log_reg, log_reg),\n",
    "    'SVM': (X_svm, svm),\n",
    "    'Decision Tree': (X_dt, dt),\n",
    "    'Random Forest': (X_rf, rf)\n",
    "}\n",
    "\n",
    "# Evaluate models after feature selection using cross-validation\n",
    "scores_after_selection = {}\n",
    "\n",
    "for name, (X_selected, model) in selected_data.items():\n",
    "    scores = cross_val_score(model, X_selected, y, cv=5)\n",
    "    scores_after_selection[name] = scores\n",
    "\n",
    "# Print the results\n",
    "print(\"Results after feature selection:\")\n",
    "for name, score in scores_after_selection.items():\n",
    "    print(f\"{name} Accuracy: {np.mean(score):.2f} ± {np.std(score):.2f}\")\n",
    "\n",
    "# Compare with the original results\n",
    "print(\"\\nComparison with original results:\")\n",
    "for name in model_names:\n",
    "    original_score = original_scores[name]\n",
    "    new_score = scores_after_selection[name]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Original: {np.mean(original_score):.2f} ± {np.std(original_score):.2f}\")\n",
    "    print(f\"  After feature selection: {np.mean(new_score):.2f} ± {np.std(new_score):.2f}\")\n",
    "    print(f\"  Difference: {np.mean(new_score) - np.mean(original_score):.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after applying PCA:\n",
      "KNN Accuracy: 0.84 ± 0.01\n",
      "Naive Bayes Accuracy: 0.78 ± 0.01\n",
      "Logistic Regression Accuracy: 0.72 ± 0.04\n",
      "SVM Accuracy: 0.80 ± 0.01\n",
      "Decision Tree Accuracy: 0.67 ± 0.02\n",
      "Random Forest Accuracy: 0.83 ± 0.03\n",
      "\n",
      "Comparison with original results:\n",
      "KNN:\n",
      "  Original: 0.49 ± 0.04\n",
      "  After PCA: 0.84 ± 0.01\n",
      "  Difference: 0.35\n",
      "\n",
      "Naive Bayes:\n",
      "  Original: 0.67 ± 0.01\n",
      "  After PCA: 0.78 ± 0.01\n",
      "  Difference: 0.11\n",
      "\n",
      "Logistic Regression:\n",
      "  Original: 0.56 ± 0.02\n",
      "  After PCA: 0.72 ± 0.04\n",
      "  Difference: 0.16\n",
      "\n",
      "SVM:\n",
      "  Original: 0.59 ± 0.03\n",
      "  After PCA: 0.80 ± 0.01\n",
      "  Difference: 0.21\n",
      "\n",
      "Decision Tree:\n",
      "  Original: 0.56 ± 0.04\n",
      "  After PCA: 0.67 ± 0.02\n",
      "  Difference: 0.11\n",
      "\n",
      "Random Forest:\n",
      "  Original: 0.73 ± 0.02\n",
      "  After PCA: 0.83 ± 0.03\n",
      "  Difference: 0.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the PCA transformation function\n",
    "def apply_pca(X, n_components=10):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca\n",
    "\n",
    "# Choose the number of principal components\n",
    "n_components = 10\n",
    "\n",
    "# Apply PCA to each model and evaluate\n",
    "pca_models = []\n",
    "pca_scores = {}\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    X_pca, pca = apply_pca(X, n_components)\n",
    "    model_pca = make_pipeline(pca, model)\n",
    "    pca_models.append(model_pca)\n",
    "    scores = cross_val_score(model_pca, X, y, cv=5)\n",
    "    pca_scores[name] = scores\n",
    "\n",
    "# Print the results after applying PCA to the models\n",
    "print(\"Results after applying PCA:\")\n",
    "for name, scores in pca_scores.items():\n",
    "    print(f\"{name} Accuracy: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\nComparison with original results:\")\n",
    "for name in model_names:\n",
    "    original_score = original_scores[name]\n",
    "    pca_score = pca_scores[name]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Original: {np.mean(original_score):.2f} ± {np.std(original_score):.2f}\")\n",
    "    print(f\"  After PCA: {np.mean(pca_score):.2f} ± {np.std(pca_score):.2f}\")\n",
    "    print(f\"  Difference: {np.mean(pca_score) - np.mean(original_score):.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My next question, which methods is better, how do you justify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common tips to justify your method in reasearch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boston",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
